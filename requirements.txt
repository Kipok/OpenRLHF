accelerate
bitsandbytes
datasets
deepspeed
einops
flash-attn
isort
jsonlines
loralib
optimum
packaging
peft
ray[default]
tensorboard
torch
torchmetrics
tqdm
transformers==4.46.0  # ring attention is broken in higher versions
transformers_stream_generator
wandb
wheel
ring_flash_attn
